{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "949c17fe-ae1e-42c2-932e-6fc8c53d7c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/09 18:00:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://p-sc-2370.2e.hpc.psu.edu:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>LDA_Tokenizer_Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x147b018043d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_LDA_Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6133be93-09c9-4feb-9861-82d16c5fbbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|text                                                            |\n",
      "+----------------------------------------------------------------+\n",
      "|This is a test document about machine learning and data science.|\n",
      "|Spark LDA is useful for topic modeling on large text corpora.   |\n",
      "|Deep learning neural networks process text data differently.    |\n",
      "|Topic modeling identifies latent themes in text documents.      |\n",
      "|Clustering and classification are core machine learning tasks.  |\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "data = [\n",
    "    Row(text=\"This is a test document about machine learning and data science.\"),\n",
    "    Row(text=\"Spark LDA is useful for topic modeling on large text corpora.\"),\n",
    "    Row(text=\"Deep learning neural networks process text data differently.\"),\n",
    "    Row(text=\"Topic modeling identifies latent themes in text documents.\"),\n",
    "    Row(text=\"Clustering and classification are core machine learning tasks.\"),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6240c78f-937e-4611-a0ea-7c91672a757a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_1f92ada587e9"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1) Tokenize text into tokens\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# 2) Remove stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# 3) Convert tokens to term-frequency vectors\n",
    "cv = CountVectorizer(\n",
    "    inputCol=\"filtered\",\n",
    "    outputCol=\"features\",\n",
    "    vocabSize=5000,\n",
    "    minDF=1    # keep terms that appear in at least 1 document\n",
    ")\n",
    "\n",
    "# 4) LDA model (k = number of topics)\n",
    "lda = LDA(\n",
    "    k=3,\n",
    "    maxIter=10,\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "# 5) Build pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, lda])\n",
    "\n",
    "pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b4756e0-ce2d-462c-8b7b-5429bc002df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/09 18:00:48 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27,\n",
       " ['text',\n",
       "  'learning',\n",
       "  'topic',\n",
       "  'data',\n",
       "  'machine',\n",
       "  'modeling',\n",
       "  'differently.',\n",
       "  'spark',\n",
       "  'deep',\n",
       "  'process',\n",
       "  'clustering',\n",
       "  'networks',\n",
       "  'useful',\n",
       "  'neural',\n",
       "  'themes',\n",
       "  'lda',\n",
       "  'classification',\n",
       "  'large',\n",
       "  'identifies',\n",
       "  'core'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipeline.fit(df)\n",
    "\n",
    "# Extract the CV and LDA sub-models\n",
    "cv_model = model.stages[2]\n",
    "lda_model = model.stages[3]\n",
    "\n",
    "vocab = cv_model.vocabulary\n",
    "len(vocab), vocab[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87d3244d-4d31-4cb5-9987-440a0d23f70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|topic|termIndices                         |termWeights                                                                                                                                                                                                             |\n",
      "+-----+------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0    |[17, 10, 13, 18, 12, 5, 7, 0, 23, 9]|[0.042917006050659935, 0.04291467482002634, 0.04171040252853162, 0.04170473250332992, 0.041315260837475526, 0.041095392612391916, 0.040482448390576055, 0.03937122377335601, 0.039211952188330894, 0.038761154927977805]|\n",
      "|1    |[7, 5, 24, 0, 2, 6, 12, 17, 15, 1]  |[0.04459437250338519, 0.04437268466228233, 0.044089174866594645, 0.04313540658490841, 0.042699932664673014, 0.04180664000866157, 0.0409891278833762, 0.040316358125679505, 0.04018598817277258, 0.03933802587288704]    |\n",
      "|2    |[26, 23, 14, 25, 2, 9, 3, 1, 22, 19]|[0.04266327600990647, 0.042296554699705544, 0.04227061442408536, 0.041990916991904645, 0.04137848042340405, 0.041086499899478206, 0.0409266143998184, 0.040866015676409614, 0.04077676184169947, 0.040183740911999916]  |\n",
      "+-----+------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Topic 0:\n",
      "  large              0.0429\n",
      "  clustering         0.0429\n",
      "  neural             0.0417\n",
      "  identifies         0.0417\n",
      "  useful             0.0413\n",
      "  modeling           0.0411\n",
      "  spark              0.0405\n",
      "  text               0.0394\n",
      "  science.           0.0392\n",
      "  process            0.0388\n",
      "\n",
      "Topic 1:\n",
      "  spark              0.0446\n",
      "  modeling           0.0444\n",
      "  corpora.           0.0441\n",
      "  text               0.0431\n",
      "  topic              0.0427\n",
      "  differently.       0.0418\n",
      "  useful             0.0410\n",
      "  large              0.0403\n",
      "  lda                0.0402\n",
      "  learning           0.0393\n",
      "\n",
      "Topic 2:\n",
      "  test               0.0427\n",
      "  science.           0.0423\n",
      "  themes             0.0423\n",
      "  documents.         0.0420\n",
      "  topic              0.0414\n",
      "  process            0.0411\n",
      "  data               0.0409\n",
      "  learning           0.0409\n",
      "  tasks.             0.0408\n",
      "  core               0.0402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_words = 10\n",
    "\n",
    "# Spark older versions require positional args\n",
    "topics = lda_model.describeTopics(num_words)\n",
    "\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Map term indices back to words and print nicely\n",
    "for row in topics.collect():\n",
    "    topic_id = row.topic\n",
    "    term_indices = row.termIndices\n",
    "    term_weights = row.termWeights\n",
    "    terms = [vocab[idx] for idx in term_indices]\n",
    "    \n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    for term, weight in zip(terms, term_weights):\n",
    "        print(f\"  {term:18s} {weight:.4f}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3302a20-df71-4d18-9948-e824278a3577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+-------------------------------------------------------------+\n",
      "|text                                                            |topicDistribution                                            |\n",
      "+----------------------------------------------------------------+-------------------------------------------------------------+\n",
      "|This is a test document about machine learning and data science.|[0.056317115516664296,0.0580467944260681,0.8856360900572676] |\n",
      "|Spark LDA is useful for topic modeling on large text corpora.   |[0.04321544726090904,0.9157818198481585,0.04100273289093252] |\n",
      "|Deep learning neural networks process text data differently.    |[0.04596807084395182,0.9070182104424673,0.047013718713580774]|\n",
      "|Topic modeling identifies latent themes in text documents.      |[0.05053219543309071,0.8990981717561027,0.05036963281080657] |\n",
      "|Clustering and classification are core machine learning tasks.  |[0.8791474780115819,0.06225199255451676,0.05860052943390128] |\n",
      "+----------------------------------------------------------------+-------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(df)\n",
    "\n",
    "# Show the text and its topic distribution\n",
    "transformed.select(\"text\", \"topicDistribution\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b6126-5bdb-4f0a-9bb3-5e85f33b4231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
