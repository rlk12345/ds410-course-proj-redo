{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619d9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147fac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 17:37:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# We create a spark session and use the spark context within the spark session because we want both dataframes and rdds\n",
    "# \n",
    "ss = SparkSession.builder.appName(\"DataProc\").config(\"spark.executor.cores\", \"24\").config(\"spark.driver.memory\", \"32g\").config(\"spark.executor.memory\", \"56g\").config(\"spark.executor.memoryOverhead\", \"32g\").getOrCreate()\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f84ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load up an rdd from the paths text file\n",
    "paths_rdd = sc.textFile(\"data/wet.paths\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72500d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea22220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take approximately sample_size paths from the samples rdd. path contains like 30k crawls so we don't want huge samples\n",
    "sample_size = 100\n",
    "sampled_paths = sc.parallelize(paths_rdd.takeSample(withReplacement=False, num=sample_size, seed=0))\n",
    "# the above example isn't exactly the most optimal sampling technique, but it allows us to be exact with sample size\n",
    "# also the strings aren't very big, and there isn't a ton of data, so this doesn't really cost us much time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c3e668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_paths.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f750ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just something we need to prepend to every path so we can create it into a link to download\n",
    "url_head = \"https://data.commoncrawl.org/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc163f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have an rdd of just urls \n",
    "urls_rdd = sampled_paths.map(lambda x: url_head + x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20595b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-47/segments/1762439342611.65/wet/CC-MAIN-20251108144350-20251108174350-00626.warc.wet.gz']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091bc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use requests to get a http response from the common crawl server\n",
    "def get_request(url): \n",
    "    import requests\n",
    "    import random\n",
    "    import time\n",
    "    \n",
    "    jitter = random.random()\n",
    "    time.sleep(jitter)\n",
    "    response = requests.get(url, stream=True) \n",
    "    if response.status_code == 200: \n",
    "        return response \n",
    "    else:\n",
    "        # Here we implement an exponential back off strategy to make sure we're not overloading the server\n",
    "        # Here we wait for a total of (2^11)/10 = 204.8 seconds or about 3.5 minutes max to get a response from the server\n",
    "        # So total wait time can be a bit over 5 minutes\n",
    "        for i in range(12):\n",
    "            jitter = random.random()\n",
    "            time.sleep((2**i + jitter)/10)\n",
    "            response = requests.get(url, stream=True)\n",
    "            if response.status_code == 200: \n",
    "                # Request was successful \n",
    "                return response \n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069a8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to have our responses RDD be filtered so we don't have any Nonetypes\n",
    "responses_rdd = urls_rdd.map(get_request).persist(StorageLevel.MEMORY_AND_DISK).filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f5d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert the responses into gzip files in memory \n",
    "def get_zipped(response): \n",
    "    import io \n",
    "    return io.BytesIO(response.content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "868d2dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[10] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_files = responses_rdd.map(get_zipped).persist(StorageLevel.MEMORY_AND_DISK) \n",
    "responses_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8254ff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 17:37:28 WARN BlockManager: Task 31 already completed, not releasing lock for rdd_9_0\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<_io.BytesIO at 0x150e6c3c7c70>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped_files.take(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "870afb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we unzip the zipped files also within memory \n",
    "def unzip(zipped): \n",
    "    import gzip \n",
    "    with gzip.GzipFile(fileobj=zipped) as decompressed_file: \n",
    "        return decompressed_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fce5680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unzipped_rdd = zipped_files.map(unzip).persist(StorageLevel.MEMORY_AND_DISK) \n",
    "zipped_files.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d8b7c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 17:37:35 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/12/07 17:37:38 WARN BlockManager: Task 32 already completed, not releasing lock for rdd_12_0\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unzipped_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c0a22e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we convert the unzipped bytes into usable strings \n",
    "str_rdd = unzipped_rdd.map(lambda x: x.decode(\"utf-8\"))\n",
    "unzipped_rdd.unpersist()\n",
    "str_rdd.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb582e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e4042dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e2888f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[14] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#str_rdd.is_cached\n",
    "str_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc82ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# as you can see this is a plaintext snippet of the plaintext WET file. we need to process this string rdd to do our analysis. \n",
    "# we need to strip away the description data and access just the text. ideally we get rid of line breaks and stuff like that too. \n",
    "# then throw it into language detection, lda, and so on \n",
    "print(str_rdd.take(1)[0][:10000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we look at the raw string and see where to split it \n",
    "str_rdd.take(1)[0][:10000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66805425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc34eb36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, True, False, False, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since there is a \\n\\r\\n between every double line break, we split it accordingly \n",
    "# We also get rid of the first 3 chunks as it essentially is just WET file metadata that exists per file; we're only interested in per-site data\n",
    "split_rdd = str_rdd.flatMap(lambda x: x.split(\"\\n\\r\\n\")[3:])\n",
    "split_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "split_rdd.getStorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3750384a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/07 17:39:06 WARN BlockManager: Task 57 already completed, not releasing lock for rdd_17_0\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "list_of_chunks = split_rdd.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2441ef5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WARC/1.0\\r\\nWARC-Type: conversion\\r\\nWARC-Target-URI: http://05xpp.com/\\r\\nWARC-Date: 2025-11-08T15:53:36Z\\r\\nWARC-Record-ID: <urn:uuid:2f166469-bf5a-46d3-89b9-9f64e5fa927f>\\r\\nWARC-Refers-To: <urn:uuid:722e0d41-d6d3-47cd-9a16-cf6f214439ee>\\r\\nWARC-Block-Digest: sha1:BFJ2KPUO7F24BQMLD52CYBHJUTAZFFY3\\r\\nWARC-Identified-Content-Language: zho\\r\\nContent-Type: text/plain\\r\\nContent-Length: 7952\\r'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_chunks[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3bae2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================================>  (23 + 1) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2266125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((split_rdd.count())//2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_rdd = str_rdd.map(lambda x: str(type(x))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118aab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_rdd = str_rdd.map(lambda x: x[0].split(\"\\n\\r\\n\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3d35fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "listed = str_rdd.take(1)[0].split(\"\\n\\r\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eae3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef26de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410)",
   "language": "python",
   "name": "ds410"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
