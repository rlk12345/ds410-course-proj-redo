{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06db02ab-152e-4c1f-888b-b95aebb7d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we are just importing neccesary packages.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# We allow for the use of either Wordcloud or Seaborn for visualization\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    HAS_WORDCLOUD = True\n",
    "except ImportError:\n",
    "    HAS_WORDCLOUD = False\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SEABORN = True\n",
    "except ImportError:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "\n",
    "\n",
    "#here we create a function to create visualzations based on the results we get from LDA\n",
    "def visualize_topics(lda_model, vocabulary, df_transformed=None, num_words=10):\n",
    "\n",
    "    topics_df = lda_model.describeTopics(maxTermsPerTopic=num_words)\n",
    "    topics = topics_df.collect()\n",
    "\n",
    "  \n",
    "    topic_labels = {}\n",
    "    for row in topics:\n",
    "        top_terms = [vocabulary[i] for i in row.termIndices[:3]]  # this is giving the top 3 words\n",
    "        label = f\"Topic {row.topic} â€” ({', '.join(top_terms)})\"\n",
    "        topic_labels[row.topic] = label\n",
    "\n",
    "    \n",
    "    \n",
    "    # Here we are creating our first visualzation: Bar Charts. \n",
    "    num_topics = len(topics)\n",
    "    fig, axes = plt.subplots(1, num_topics, figsize=(5 * num_topics, 6))\n",
    "    if num_topics == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, topic_row in enumerate(topics):\n",
    "        term_indices = topic_row.termIndices\n",
    "        term_weights = topic_row.termWeights\n",
    "        words = [vocabulary[i] for i in term_indices]\n",
    "        weights = [float(w) for w in term_weights]\n",
    "\n",
    "        ax = axes[idx]\n",
    "        ax.barh(words, weights)\n",
    "        ax.set_title(topic_labels[topic_row.topic])\n",
    "        ax.set_xlabel(\"Weight\")\n",
    "        ax.invert_yaxis() # we do this so that the highest weight is at the top.\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Here we are creating our second  visualzation:Word Clouds. \n",
    "    if HAS_WORDCLOUD:\n",
    "        fig, axes = plt.subplots(1, num_topics, figsize=(5 * num_topics, 4))\n",
    "        if num_topics == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for idx, topic_row in enumerate(topics):\n",
    "            term_indices = topic_row.termIndices\n",
    "            term_weights = topic_row.termWeights\n",
    "            word_freq = {vocabulary[i]: float(w) for i, w in zip(term_indices, term_weights)}\n",
    "\n",
    "            wordcloud = WordCloud(width=425, height=325, background_color=\"white\") \\\n",
    "                .generate_from_frequencies(word_freq)\n",
    "\n",
    "            axes[idx].imshow(wordcloud, interpolation=\"bilinear\")\n",
    "            axes[idx].set_title(topic_labels[topic_row.topic])\n",
    "            axes[idx].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Here we are creating our third visualzation: Document Topic Distributions. \n",
    "    if df_transformed is not None:\n",
    "        \n",
    "     # we want to extract the topic distribution column from the df which is containing the vector of probabilites   \n",
    "        rows = df_transformed.select(\"topicDistribution\").collect()\n",
    "        topic_probs = []\n",
    "        for row in rows:\n",
    "            dist = row.topicDistribution\n",
    "            \n",
    "            if isinstance(dist, (DenseVector, SparseVector)):\n",
    "                dist = dist.toArray()\n",
    "            topic_probs.append([float(x) for x in dist])\n",
    "\n",
    "        df_topics = pd.DataFrame(\n",
    "            topic_probs,\n",
    "            columns=[topic_labels[i] for i in range(num_topics)]\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        topic_means = df_topics.mean()\n",
    "        ax.bar(topic_means.index, topic_means.values)\n",
    "        ax.set_ylabel(\"Average Probability\")\n",
    "        ax.set_title(\"Average Topic Proportions\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if HAS_SEABORN:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            sns.heatmap(df_topics.T, cmap=\"YlOrRd\", cbar_kws={\"label\": \"Probability\"}, ax=ax)\n",
    "            ax.set_xlabel(\"Document\")\n",
    "            ax.set_ylabel(\"Topic\")\n",
    "            ax.set_title(\"Document-Topic Distribution\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Here we are calling the visualizations. Uncomment when you want to employ it.\n",
    "# visualize_topics(lda_model, vocab, transformed, num_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de21622-21d4-4aa6-8237-7d7580bacf25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds410_f25)",
   "language": "python",
   "name": "ds410_f25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
